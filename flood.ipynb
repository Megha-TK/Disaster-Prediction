{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:/Users/harsh/Downloads/FloodPrediction.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mc:/Users/harsh/Downloads/FloodPrediction.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Fill missing values in 'Flood?' column with 0 (No Flood)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFlood?\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFlood?\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\incha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:/Users/harsh/Downloads/FloodPrediction.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to get latitude and longitude of a location\n",
    "def get_lat_lon(api_key, location):\n",
    "    geo_url = f\"https://api.openweathermap.org/geo/1.0/direct?q={location}&limit=1&appid={api_key}\"\n",
    "    try:\n",
    "        response = requests.get(geo_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if data:\n",
    "            return data[0][\"lat\"], data[0][\"lon\"]\n",
    "        else:\n",
    "            print(\"Location not found.\")\n",
    "            return None, None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching location data: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Function to get current weather data using OpenWeatherMap API\n",
    "def get_weather_data(api_key, lat, lon):\n",
    "    url = f\"https://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={api_key}&units=metric\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return {\n",
    "            \"Max_Temp\": data[\"main\"][\"temp\"],\n",
    "            \"Rainfall\": data.get(\"rain\", {}).get(\"1h\", 0),\n",
    "            \"Relative_Humidity\": data[\"main\"][\"humidity\"],\n",
    "            \"Wind_Speed\": data[\"wind\"][\"speed\"],\n",
    "            \"Cloud_Coverage\": data[\"clouds\"][\"all\"],\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching weather data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:/Users/incha/Downloads/FloodPrediction.csv\")\n",
    "\n",
    "# Fill missing values in 'Flood?' column with 0 (No Flood)\n",
    "df['Flood?'] = df['Flood?'].fillna(0)\n",
    "\n",
    "# Selecting relevant features\n",
    "features = [\"Max_Temp\", \"Rainfall\", \"Relative_Humidity\", \"Wind_Speed\", \"Cloud_Coverage\"]\n",
    "X = df[features]\n",
    "\n",
    "# Target variables\n",
    "y_flood = df[\"Flood?\"]\n",
    "\n",
    "# Encoding categorical target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_flood = label_encoder.fit_transform(y_flood)\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_flood, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizing data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Building the neural network model\n",
    "model = Sequential([\n",
    "    keras.Input(shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9080 - loss: 0.2603 - val_accuracy: 0.9465 - val_loss: 0.1430\n",
      "Epoch 2/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9367 - loss: 0.1642 - val_accuracy: 0.9498 - val_loss: 0.1380\n",
      "Epoch 3/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9377 - loss: 0.1635 - val_accuracy: 0.9492 - val_loss: 0.1401\n",
      "Epoch 4/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9403 - loss: 0.1582 - val_accuracy: 0.9480 - val_loss: 0.1400\n",
      "Epoch 5/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9433 - loss: 0.1481 - val_accuracy: 0.9483 - val_loss: 0.1389\n",
      "Epoch 6/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9416 - loss: 0.1509 - val_accuracy: 0.9462 - val_loss: 0.1408\n",
      "Epoch 7/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9386 - loss: 0.1547 - val_accuracy: 0.9465 - val_loss: 0.1412\n",
      "Epoch 8/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9443 - loss: 0.1502 - val_accuracy: 0.9471 - val_loss: 0.1387\n",
      "Epoch 9/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9451 - loss: 0.1442 - val_accuracy: 0.9480 - val_loss: 0.1388\n",
      "Epoch 10/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9427 - loss: 0.1480 - val_accuracy: 0.9440 - val_loss: 0.1413\n",
      "Epoch 11/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9431 - loss: 0.1519 - val_accuracy: 0.9474 - val_loss: 0.1387\n",
      "Epoch 12/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9428 - loss: 0.1494 - val_accuracy: 0.9434 - val_loss: 0.1411\n",
      "Epoch 13/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9427 - loss: 0.1480 - val_accuracy: 0.9483 - val_loss: 0.1381\n",
      "Epoch 14/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9440 - loss: 0.1429 - val_accuracy: 0.9443 - val_loss: 0.1418\n",
      "Epoch 15/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9432 - loss: 0.1536 - val_accuracy: 0.9480 - val_loss: 0.1385\n",
      "Epoch 16/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9465 - loss: 0.1474 - val_accuracy: 0.9495 - val_loss: 0.1387\n",
      "Epoch 17/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9418 - loss: 0.1522 - val_accuracy: 0.9462 - val_loss: 0.1387\n",
      "Epoch 18/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9410 - loss: 0.1517 - val_accuracy: 0.9477 - val_loss: 0.1382\n",
      "Epoch 19/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9386 - loss: 0.1520 - val_accuracy: 0.9471 - val_loss: 0.1385\n",
      "Epoch 20/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9450 - loss: 0.1418 - val_accuracy: 0.9477 - val_loss: 0.1386\n",
      "Epoch 21/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9435 - loss: 0.1491 - val_accuracy: 0.9452 - val_loss: 0.1412\n",
      "Epoch 22/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9422 - loss: 0.1461 - val_accuracy: 0.9483 - val_loss: 0.1368\n",
      "Epoch 23/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9426 - loss: 0.1525 - val_accuracy: 0.9462 - val_loss: 0.1383\n",
      "Epoch 24/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9442 - loss: 0.1441 - val_accuracy: 0.9480 - val_loss: 0.1374\n",
      "Epoch 25/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9432 - loss: 0.1461 - val_accuracy: 0.9474 - val_loss: 0.1371\n",
      "Epoch 26/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9439 - loss: 0.1433 - val_accuracy: 0.9468 - val_loss: 0.1373\n",
      "Epoch 27/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9467 - loss: 0.1439 - val_accuracy: 0.9471 - val_loss: 0.1358\n",
      "Epoch 28/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9429 - loss: 0.1504 - val_accuracy: 0.9471 - val_loss: 0.1365\n",
      "Epoch 29/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9439 - loss: 0.1427 - val_accuracy: 0.9474 - val_loss: 0.1372\n",
      "Epoch 30/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9431 - loss: 0.1452 - val_accuracy: 0.9480 - val_loss: 0.1369\n",
      "Epoch 31/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9442 - loss: 0.1478 - val_accuracy: 0.9471 - val_loss: 0.1389\n",
      "Epoch 32/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9435 - loss: 0.1396 - val_accuracy: 0.9477 - val_loss: 0.1379\n",
      "Epoch 33/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9452 - loss: 0.1397 - val_accuracy: 0.9477 - val_loss: 0.1383\n",
      "Epoch 34/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9437 - loss: 0.1457 - val_accuracy: 0.9474 - val_loss: 0.1373\n",
      "Epoch 35/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9441 - loss: 0.1489 - val_accuracy: 0.9492 - val_loss: 0.1382\n",
      "Epoch 36/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9425 - loss: 0.1441 - val_accuracy: 0.9480 - val_loss: 0.1377\n",
      "Epoch 37/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9452 - loss: 0.1422 - val_accuracy: 0.9480 - val_loss: 0.1385\n",
      "Epoch 38/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9358 - loss: 0.1542 - val_accuracy: 0.9477 - val_loss: 0.1370\n",
      "Epoch 39/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9416 - loss: 0.1488 - val_accuracy: 0.9483 - val_loss: 0.1359\n",
      "Epoch 40/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9458 - loss: 0.1427 - val_accuracy: 0.9458 - val_loss: 0.1374\n",
      "Epoch 41/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9437 - loss: 0.1461 - val_accuracy: 0.9489 - val_loss: 0.1382\n",
      "Epoch 42/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9453 - loss: 0.1416 - val_accuracy: 0.9468 - val_loss: 0.1376\n",
      "Epoch 43/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9431 - loss: 0.1452 - val_accuracy: 0.9480 - val_loss: 0.1358\n",
      "Epoch 44/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9466 - loss: 0.1370 - val_accuracy: 0.9495 - val_loss: 0.1365\n",
      "Epoch 45/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9462 - loss: 0.1387 - val_accuracy: 0.9471 - val_loss: 0.1395\n",
      "Epoch 46/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9474 - loss: 0.1400 - val_accuracy: 0.9474 - val_loss: 0.1376\n",
      "Epoch 47/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9421 - loss: 0.1437 - val_accuracy: 0.9465 - val_loss: 0.1388\n",
      "Epoch 48/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9435 - loss: 0.1462 - val_accuracy: 0.9486 - val_loss: 0.1360\n",
      "Epoch 49/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9436 - loss: 0.1448 - val_accuracy: 0.9443 - val_loss: 0.1418\n",
      "Epoch 50/50\n",
      "\u001b[1m822/822\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9447 - loss: 0.1436 - val_accuracy: 0.9480 - val_loss: 0.1362\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step\n",
      "Model Accuracy: 94.35%\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save(r\"C:\\Users\\harsh\\OneDrive\\Desktop\\flood_prediction\\flood_prediction_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_flood(api_key):\n",
    "    location = input(\"Enter city or state name: \")\n",
    "    lat, lon = get_lat_lon(api_key, location)\n",
    "    if lat is not None and lon is not None:\n",
    "        print(f\"Location: {location}, Latitude: {lat}, Longitude: {lon}\")\n",
    "        weather_data = get_weather_data(api_key, lat, lon)\n",
    "        if weather_data:\n",
    "            weather_df = pd.DataFrame([weather_data])\n",
    "            weather_scaled = scaler.transform(weather_df)\n",
    "            prediction = model.predict(weather_scaled)[0][0]\n",
    "            if prediction > 0.7:\n",
    "                flood_risk = \"High Risk Flood Area\"\n",
    "            elif prediction > 0.4:\n",
    "                flood_risk = \"Moderate Risk Area\"\n",
    "            else:\n",
    "                flood_risk = \"Flood Free Area\"\n",
    "            print(f\"Flood Prediction for {location}: {flood_risk}\")\n",
    "        else:\n",
    "            print(\"Failed to retrieve weather data.\")\n",
    "    else:\n",
    "        print(\"Failed to fetch latitude and longitude.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location: mysuru, Latitude: 12.3051828, Longitude: 76.6553609\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Cloud_Coverage\n- Max_Temp\n- Rainfall\n- Relative_Humidity\n- Wind_Speed\nFeature names seen at fit time, yet now missing:\n- Depth\n- Latitude\n- Location\n- Longitude\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me3a90990b56034cdb755847b6439abae\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mpredict_flood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[84], line 9\u001b[0m, in \u001b[0;36mpredict_flood\u001b[1;34m(api_key)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weather_data:\n\u001b[0;32m      8\u001b[0m     weather_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([weather_data])\n\u001b[1;32m----> 9\u001b[0m     weather_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweather_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(weather_scaled)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prediction \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.7\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1045\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1042\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1044\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1045\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Cloud_Coverage\n- Max_Temp\n- Rainfall\n- Relative_Humidity\n- Wind_Speed\nFeature names seen at fit time, yet now missing:\n- Depth\n- Latitude\n- Location\n- Longitude\n"
     ]
    }
   ],
   "source": [
    "api_key = \"e3a90990b56034cdb755847b6439abae\"\n",
    "predict_flood(api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📍 Location: Mumbai, Latitude: 19.0785451, Longitude: 72.878176\n",
      "📊 Model Input Data Before Scaling:\n",
      "   Max_Temp  Min_Temp  Rainfall  Relative_Humidity  Wind_Speed  \\\n",
      "0     31.28     27.78         0                 32        6.48   \n",
      "1     29.20     27.28         0                 37        2.90   \n",
      "2     26.04     26.04         0                 48        3.60   \n",
      "3     25.24     25.24         0                 49        3.76   \n",
      "4     25.69     25.69         0                 42        2.79   \n",
      "\n",
      "   Cloud_Coverage   Latitude  Longitude     Depth  Location  \n",
      "0               3  19.078545  72.878176  5.638142       119  \n",
      "1               2  19.078545  72.878176  8.988548       119  \n",
      "2               0  19.078545  72.878176  7.291736       119  \n",
      "3               0  19.078545  72.878176  7.697227       119  \n",
      "4               0  19.078545  72.878176  9.211237       119  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Cloud_Coverage\n- Max_Temp\n- Min_Temp\n- Rainfall\n- Relative_Humidity\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 97\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Run the flood prediction function\u001b[39;00m\n\u001b[0;32m     96\u001b[0m API_KEY \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me3a90990b56034cdb755847b6439abae\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 97\u001b[0m \u001b[43mpredict_flood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAPI_KEY\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[79], line 66\u001b[0m, in \u001b[0;36mpredict_flood\u001b[1;34m(api_key)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(weather_df\u001b[38;5;241m.\u001b[39mhead())  \u001b[38;5;66;03m# Debugging print\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Scale the weather data\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m weather_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweather_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📊 Scaled Data Before Prediction:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(weather_scaled[:\u001b[38;5;241m5\u001b[39m])  \u001b[38;5;66;03m# Debugging print\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1045\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1042\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1044\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1045\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Cloud_Coverage\n- Max_Temp\n- Min_Temp\n- Rainfall\n- Relative_Humidity\n- ...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Load the trained flood prediction model (.h5 file)\n",
    "model = load_model(\"C:/Users/harsh/OneDrive/Desktop/flood_prediction/flood_prediction_model.h5\")\n",
    "\n",
    "# Load the pre-trained scaler\n",
    "scaler = joblib.load(\"C:/Users/harsh/OneDrive/Desktop/flood_predictionscaler.pkl\")\n",
    "\n",
    "def get_lat_lon(api_key, location):\n",
    "    \"\"\"Fetch latitude and longitude for a given location.\"\"\"\n",
    "    url = f\"http://api.openweathermap.org/geo/1.0/direct?q={location}&limit=1&appid={api_key}\"\n",
    "    response = requests.get(url).json()\n",
    "\n",
    "    if response and isinstance(response, list) and len(response) > 0:\n",
    "        lat = response[0]['lat']\n",
    "        lon = response[0]['lon']\n",
    "        return lat, lon\n",
    "    return None, None\n",
    "\n",
    "def get_weather_forecast(api_key, lat, lon):\n",
    "    \"\"\"Fetch 10-day weather forecast data from OpenWeather API.\"\"\"\n",
    "    url = f\"https://api.openweathermap.org/data/2.5/forecast?lat={lat}&lon={lon}&units=metric&cnt=10&appid={api_key}\"\n",
    "    response = requests.get(url).json()\n",
    "\n",
    "    if \"list\" in response:\n",
    "        forecast_data = []\n",
    "        for day in response[\"list\"]:\n",
    "            forecast_data.append({\n",
    "                \"Max_Temp\": day[\"main\"][\"temp_max\"],  # Max temperature\n",
    "                \"Min_Temp\": day[\"main\"][\"temp_min\"],  # Min temperature\n",
    "                \"Rainfall\": day.get(\"rain\", {}).get(\"3h\", 0),  # Rainfall (default 0 if missing)\n",
    "                \"Relative_Humidity\": day[\"main\"][\"humidity\"],  # Humidity\n",
    "                \"Wind_Speed\": day[\"wind\"][\"speed\"],  # Wind speed\n",
    "                \"Cloud_Coverage\": day[\"clouds\"][\"all\"],  # Cloud coverage\n",
    "                \"Latitude\": lat,  # Add Latitude\n",
    "                \"Longitude\": lon,  # Add Longitude\n",
    "                \"Depth\": np.random.uniform(1, 10),  # Simulated depth (If your model needs it)\n",
    "                \"Location\": hash(location) % 1000  # Encode location as a number (If necessary)\n",
    "            })\n",
    "        return forecast_data\n",
    "    return None\n",
    "\n",
    "def predict_flood(api_key):\n",
    "    \"\"\"Predict flood risk for the next 10 days using the trained model.\"\"\"\n",
    "    location = input(\"Enter city or state name: \")\n",
    "    lat, lon = get_lat_lon(api_key, location)\n",
    "\n",
    "    if lat is not None and lon is not None:\n",
    "        print(f\"\\n📍 Location: {location}, Latitude: {lat}, Longitude: {lon}\")\n",
    "\n",
    "        weather_forecast = get_weather_forecast(api_key, lat, lon)\n",
    "        if weather_forecast:\n",
    "            # Convert forecast data to DataFrame\n",
    "            weather_df = pd.DataFrame(weather_forecast)\n",
    "\n",
    "            print(\"📊 Model Input Data Before Scaling:\")\n",
    "            print(weather_df.head())  # Debugging print\n",
    "\n",
    "            # Scale the weather data\n",
    "            weather_scaled = scaler.transform(weather_df)\n",
    "\n",
    "            print(\"📊 Scaled Data Before Prediction:\")\n",
    "            print(weather_scaled[:5])  # Debugging print\n",
    "\n",
    "            # Predict flood risk for the next 10 days\n",
    "            predictions = model.predict(weather_scaled).flatten()\n",
    "\n",
    "            # Interpret predictions\n",
    "            flood_predictions = []\n",
    "            for i, prediction in enumerate(predictions):\n",
    "                if prediction > 0.7:\n",
    "                    flood_risk = \"High Risk Flood Area\"\n",
    "                elif prediction > 0.4:\n",
    "                    flood_risk = \"Moderate Risk Area\"\n",
    "                else:\n",
    "                    flood_risk = \"Flood Free Area\"\n",
    "\n",
    "                flood_predictions.append(f\"Day {i+1}: {flood_risk}\")\n",
    "\n",
    "            # Print predictions\n",
    "            print(\"\\n🌊 Flood Prediction for the Next 10 Days:\")\n",
    "            for pred in flood_predictions:\n",
    "                print(pred)\n",
    "        else:\n",
    "            print(\"❌ Failed to retrieve 10-day weather forecast.\")\n",
    "    else:\n",
    "        print(\"❌ Failed to fetch latitude and longitude.\")\n",
    "\n",
    "# Run the flood prediction function\n",
    "API_KEY = \"e3a90990b56034cdb755847b6439abae\"\n",
    "predict_flood(API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📍 Location: Mumbai, Latitude: 19.0785451, Longitude: 72.878176\n",
      "\n",
      "✅ Expected Features: ['Latitude' 'Longitude' 'Depth' 'Location']\n",
      "\n",
      "✅ Actual Features in Weather Data: ['Date', 'Max_Temp', 'Min_Temp', 'Rainfall', 'Relative_Humidity', 'Wind_Speed', 'Cloud_Coverage', 'Latitude', 'Longitude', 'Depth', 'Location']\n",
      "\n",
      "📊 Model Input Data Before Scaling:\n",
      "    Latitude  Longitude     Depth  Location\n",
      "0  19.078545  72.878176  2.524066       119\n",
      "1  19.078545  72.878176  2.607219       119\n",
      "2  19.078545  72.878176  4.251756       119\n",
      "3  19.078545  72.878176  2.359132       119\n",
      "4  19.078545  72.878176  1.689561       119\n",
      "\n",
      "📊 Scaled Data Before Prediction:\n",
      "[[-1.48015717 -0.79502645 -0.73590223 -1.5190145 ]\n",
      " [-1.48015717 -0.79502645 -0.73469005 -1.5190145 ]\n",
      " [-1.48015717 -0.79502645 -0.71071646 -1.5190145 ]\n",
      " [-1.48015717 -0.79502645 -0.7383066  -1.5190145 ]\n",
      " [-1.48015717 -0.79502645 -0.74806741 -1.5190145 ]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_9\" is incompatible with the layer: expected axis -1 of input shape to have value 5, but received input with shape (10, 4)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(10, 4), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 109\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Run the flood prediction function\u001b[39;00m\n\u001b[0;32m    108\u001b[0m API_KEY \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc5615a8df07bf39f21e624ffe731f157\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 109\u001b[0m \u001b[43mpredict_flood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAPI_KEY\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[83], line 84\u001b[0m, in \u001b[0;36mpredict_flood\u001b[1;34m(api_key)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(weather_scaled[:\u001b[38;5;241m5\u001b[39m])  \n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Predict flood risk for the next 10 days\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweather_scaled\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Interpret predictions\u001b[39;00m\n\u001b[0;32m     87\u001b[0m flood_predictions \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    224\u001b[0m             value,\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         }:\n\u001b[1;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_9\" is incompatible with the layer: expected axis -1 of input shape to have value 5, but received input with shape (10, 4)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(10, 4), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from datetime import datetime, timezone, timedelta\n",
    "# Load the trained flood prediction model (.h5 file)\n",
    "model = load_model(\"C:/Users/harsh/OneDrive/Desktop/flood_prediction/flood_prediction_model.h5\")\n",
    "\n",
    "# Load the pre-trained scaler\n",
    "scaler = joblib.load(\"C:/Users/harsh/OneDrive/Desktop/flood_prediction/scaler.pkl\")\n",
    "\n",
    "def get_lat_lon(api_key, location):\n",
    "    \"\"\"Fetch latitude and longitude for a given location.\"\"\"\n",
    "    url = f\"http://api.openweathermap.org/geo/1.0/direct?q={location}&limit=1&appid={api_key}\"\n",
    "    response = requests.get(url).json()\n",
    "\n",
    "    if response and isinstance(response, list) and len(response) > 0:\n",
    "        lat = response[0]['lat']\n",
    "        lon = response[0]['lon']\n",
    "        return lat, lon\n",
    "    return None, None\n",
    "\n",
    "def get_weather_forecast(api_key, lat, lon):\n",
    "    \"\"\"Fetch 10-day weather forecast data from OpenWeather API.\"\"\"\n",
    "    url = f\"https://api.openweathermap.org/data/2.5/forecast?lat={lat}&lon={lon}&units=metric&cnt=10&appid={api_key}\"\n",
    "    response = requests.get(url).json()\n",
    "\n",
    "    if \"list\" in response:\n",
    "        forecast_data = []\n",
    "        for day in response[\"list\"]:\n",
    "            timestamp = day[\"dt\"]\n",
    "            date = datetime.fromtimestamp(timestamp, timezone.utc).strftime('%Y-%m-%d')  # Fix DeprecationWarning\n",
    "\n",
    "            forecast_data.append({\n",
    "                \"Date\": date,\n",
    "                \"Max_Temp\": day[\"main\"][\"temp_max\"],  \n",
    "                \"Min_Temp\": day[\"main\"][\"temp_min\"],  \n",
    "                \"Rainfall\": day.get(\"rain\", {}).get(\"3h\", 0),  \n",
    "                \"Relative_Humidity\": day[\"main\"][\"humidity\"],  \n",
    "                \"Wind_Speed\": day[\"wind\"][\"speed\"],  \n",
    "                \"Cloud_Coverage\": day[\"clouds\"][\"all\"],  \n",
    "                \"Latitude\": lat,  # Add Latitude\n",
    "                \"Longitude\": lon,  # Add Longitude\n",
    "                \"Depth\": np.random.uniform(1, 10),  # Simulated depth (If your model needs it)\n",
    "                \"Location\": hash(location) % 1000  # Encode location as a number (If necessary)\n",
    "            })\n",
    "        return forecast_data\n",
    "    return None\n",
    "\n",
    "def predict_flood(api_key):\n",
    "    \"\"\"Predict flood risk for the next 10 days using the trained model.\"\"\"\n",
    "    location = input(\"Enter city or state name: \")\n",
    "    lat, lon = get_lat_lon(api_key, location)\n",
    "\n",
    "    if lat is not None and lon is not None:\n",
    "        print(f\"\\n📍 Location: {location}, Latitude: {lat}, Longitude: {lon}\")\n",
    "\n",
    "        weather_forecast = get_weather_forecast(api_key, lat, lon)\n",
    "        if weather_forecast:\n",
    "            # Convert forecast data to DataFrame\n",
    "            weather_df = pd.DataFrame(weather_forecast)\n",
    "\n",
    "            # Print expected vs actual feature names\n",
    "            expected_columns = scaler.feature_names_in_\n",
    "            print(\"\\n✅ Expected Features:\", expected_columns)\n",
    "            print(\"\\n✅ Actual Features in Weather Data:\", weather_df.columns.tolist())\n",
    "\n",
    "            # Ensure feature order matches the trained model\n",
    "            weather_df = weather_df[expected_columns]\n",
    "\n",
    "            print(\"\\n📊 Model Input Data Before Scaling:\")\n",
    "            print(weather_df.head())  \n",
    "\n",
    "            # Scale the weather data\n",
    "            weather_scaled = scaler.transform(weather_df)\n",
    "\n",
    "            print(\"\\n📊 Scaled Data Before Prediction:\")\n",
    "            print(weather_scaled[:5])  \n",
    "\n",
    "            # Predict flood risk for the next 10 days\n",
    "            predictions = model.predict(weather_scaled).flatten()\n",
    "\n",
    "            # Interpret predictions\n",
    "            flood_predictions = []\n",
    "            for i, prediction in enumerate(predictions):\n",
    "                if prediction > 0.7:\n",
    "                    flood_risk = \"High Risk Flood Area\"\n",
    "                elif prediction > 0.4:\n",
    "                    flood_risk = \"Moderate Risk Area\"\n",
    "                else:\n",
    "                    flood_risk = \"Flood Free Area\"\n",
    "\n",
    "                flood_predictions.append(f\"Day {i+1}: {flood_risk}\")\n",
    "\n",
    "            # Print predictions\n",
    "            print(\"\\n🌊 Flood Prediction for the Next 10 Days:\")\n",
    "            for pred in flood_predictions:\n",
    "                print(pred)\n",
    "        else:\n",
    "            print(\"❌ Failed to retrieve 10-day weather forecast.\")\n",
    "    else:\n",
    "        print(\"❌ Failed to fetch latitude and longitude.\")\n",
    "\n",
    "# Run the flood prediction function\n",
    "API_KEY = \"c5615a8df07bf39f21e624ffe731f157\"\n",
    "predict_flood(API_KEY)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
